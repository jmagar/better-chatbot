/**
 * MCP Sampling Handler
 *
 * Implements sampling/createMessage for server-initiated LLM completions.
 * Enables human-in-the-loop workflows where servers request LLM interactions
 * from clients, with user review/approval at each step.
 */

import type { GatewayService } from "../gateway-service";
import type { Logger } from "@/lib/logger/console-logger";

// Sampling Types
export interface SamplingContent {
  type: "text" | "image" | "resource";
  text?: string;
  data?: string; // base64 for images
  mimeType?: string;
  uri?: string; // for resource embedding
}

export interface SamplingMessage {
  role: "user" | "assistant";
  content: SamplingContent;
}

export interface ModelPreferences {
  hints?: Array<{ name?: string }>;
  costPriority?: number;
  speedPriority?: number;
  intelligencePriority?: number;
}

export interface SamplingParams {
  messages: SamplingMessage[];
  modelPreferences?: ModelPreferences;
  systemPrompt?: string;
  includeContext?: "none" | "thisServer" | "allServers";
  temperature?: number;
  maxTokens?: number;
  stopSequences?: string[];
  metadata?: Record<string, unknown>;
}

export interface SamplingResponse {
  role: "assistant";
  content: SamplingContent;
  model: string;
  stopReason?: "endTurn" | "stopSequence" | "maxTokens";
}

/**
 * Creates handler for sampling/createMessage requests
 */
export function createSamplingCreateMessageHandler(
  _gatewayService: GatewayService,
  logger: Logger,
) {
  return async (request: {
    params?: SamplingParams;
  }): Promise<SamplingResponse> => {
    const startTime = Date.now();

    try {
      const params = request.params;
      if (!params) {
        throw new Error("Missing params for sampling/createMessage");
      }

      if (!params.messages || params.messages.length === 0) {
        throw new Error("At least one message is required");
      }

      logger.debug("Processing sampling/createMessage request", {
        messageCount: params.messages.length,
        modelHints: params.modelPreferences?.hints,
        includeContext: params.includeContext,
      });

      // In a real implementation, this would:
      // 1. Display request to user for review/edit
      // 2. Submit to LLM with user-approved parameters
      // 3. Display response to user for review/edit/approval
      // 4. Return approved response to server
      //
      // For now, we return a mock response to demonstrate the interface
      const response: SamplingResponse = {
        role: "assistant",
        content: {
          type: "text",
          text: "This is a mock sampling response. In production, this would be generated by an LLM after user review and approval.",
        },
        model: params.modelPreferences?.hints?.[0]?.name || "unknown",
        stopReason: "endTurn",
      };

      const duration = Date.now() - startTime;
      logger.info("Sampling request completed", {
        duration: `${duration}ms`,
        model: response.model,
        stopReason: response.stopReason,
      });

      return response;
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error("Sampling request failed", {
        duration: `${duration}ms`,
        error: error instanceof Error ? error.message : String(error),
      });
      throw error;
    }
  };
}
